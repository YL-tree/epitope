{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49150b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'esm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_one\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BepiPredDDPM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ESM2Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32md:\\yl\\b_cell\\model_one.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'esm'"
     ]
    }
   ],
   "source": [
    "from model_one import BepiPredDDPM\n",
    "from data import ESM2Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(418)  # 设置种子值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84862664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not detected. Using CPU: cpu\n"
     ]
    }
   ],
   "source": [
    "### SET GPU OR CPU ###\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU device detected: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"GPU device not detected. Using CPU: {device}\")\n",
    "    \n",
    "\n",
    "\n",
    "def get_alpha_cumprod(t, steps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "    \"\"\"\n",
    "    计算alpha的累计乘积\n",
    "    \"\"\"\n",
    "    beta = torch.linspace(beta_start, beta_end, steps)\n",
    "    alpha = 1 - beta\n",
    "    alpha_cumprod = torch.cumprod(alpha, dim=0)\n",
    "    return alpha_cumprod[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集并加载数据集\n",
    "esm_encoding_dir=Path(\"data/esm_encodings\")\n",
    "train_esm_encoding_dir, test_esm_encoding_dir = train_test_split(esm_encoding_dir, test_size=0.2, random_state=42)\n",
    "train_dataset = ESM2Dataset(train_esm_encoding_dir)\n",
    "test_dataset = ESM2Dataset(test_esm_encoding_dir)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 每个样本是 (input_tensor, target_tensor)\n",
    "    input_seqs = [item[0] for item in batch]\n",
    "    target_seqs = [item[1] for item in batch]\n",
    "\n",
    "    input_lengths = torch.tensor([len(seq) for seq in input_seqs])\n",
    "    target_lengths = torch.tensor([len(seq) for seq in target_seqs])\n",
    "\n",
    "    # 两个序列的长度是一样的，可以统一 pad 成 max_len\n",
    "    # 你也可以分别 pad（如果以后变成不一样长）\n",
    "\n",
    "    input_padded = pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_padded = pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return input_padded, input_lengths, target_padded, target_lengths\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "for esm, epitope, esm_len, epitope_len in train_dataloader:\n",
    "    print(esm.shape)\n",
    "    print(epitope.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint_dir = './checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)\n",
    "    print(f'Checkpoint saved to {checkpoint_path}')\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d32c89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model = BepiPredDDPM()\n",
    "diffusion = Diffusion()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0430312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 500, 1281])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mb/58trsstd19510mh5cpw6ckfw0000gn/T/ipykernel_59541/929776122.py:23: UserWarning: Using a target size (torch.Size([20, 500, 1281])) that is different to the input size (torch.Size([10000, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_diffusion = F.mse_loss(pred_noise, noise)\n",
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 500, 1281])\n",
      "torch.Size([20, 500, 1281])\n",
      "torch.Size([20, 500, 1281])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (1281) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 3. 预测噪声并计算损失\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pred_noise, epitope_prob \u001b[38;5;241m=\u001b[39m model(xt, t)\n\u001b[0;32m---> 23\u001b[0m loss_diffusion \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_noise, noise)\n\u001b[1;32m     24\u001b[0m loss_epitope \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(epitope_prob, epitope_labels)  \u001b[38;5;66;03m# 需提供真实表位标签\u001b[39;00m\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_diffusion \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m loss_epitope  \u001b[38;5;66;03m# 加权平衡\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (1281) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# def train(model, dataloader, optimizer, steps=1000, device=device, epochs=10):\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "epochs_losses = []  # 记录每个epoch的平均损失\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "    loss_record= []\n",
    "    for x0, epitope_labels, x0_len, epitope_len in progress_bar:  # x0: 真实ESM嵌入 [B, L, D]\n",
    "        print(x0.shape)\n",
    "        \n",
    "        # epitope_labels = epitope_labels.to(device)\n",
    "        # x0 = x0.to(device)\n",
    "        \n",
    "        # 1. 随机采样时间步和噪声\n",
    "        t = torch.randint(0, steps, (x0.size(0),), device=device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        \n",
    "        # 2. 前向加噪（根据噪声调度）\n",
    "        \n",
    "        xt = diffusion.q_sample(x0, t)\n",
    "        t = t.float()\n",
    "        # 3. 预测噪声并计算损失\n",
    "        pred_noise, epitope_prob = model(xt, t)\n",
    "\n",
    "        # 4. 计算表位分类损失\n",
    "        loss_diffusion = F.mse_loss(pred_noise, noise)\n",
    "        loss_epitope = F.binary_cross_entropy(epitope_prob, epitope_labels)  # 需提供真实表位标签\n",
    "        total_loss = 0.3 * loss_diffusion + 0.7 * loss_epitope  # 加权平衡\n",
    "        \n",
    "        # 4. 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        loss_record.append(total_loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. 记录损失\n",
    "        progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "        epoch_loss = torch.tensor(loss_record).mean().item()\n",
    "        epochs_losses.append(epoch_loss)\n",
    "        \n",
    "    # 每15个epoch保存一次checkpoint\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, epoch_loss, checkpoint_dir)\n",
    "        print(f\"Epoch {epoch + 1}, Loss_Mean: {epoch_loss}\", end=\"\\r\")\n",
    "        plt.plot(epochs_losses)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 500, 1281])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, dataloader, device=device):\n",
    "    probs, labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x0, epitope_labels in dataloader:\n",
    "            _, epitope_prob = model(x0.to(device), torch.zeros(x0.size(0)).to(device))\n",
    "            probs.append(epitope_prob.cpu())\n",
    "            labels.append(epitope_labels.cpu())\n",
    "    \n",
    "    probs = torch.cat(probs).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "    precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "    pr_auc = auc(recall, precision)  # 更关注正类的指标\n",
    "    return pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7bb686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
