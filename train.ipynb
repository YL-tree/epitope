{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49150b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_one import BepiPredDDPM, ESM2Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "random.seed(418)  # 设置种子值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84862664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not detected. Using CPU: cpu\n"
     ]
    }
   ],
   "source": [
    "### SET GPU OR CPU ###\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU device detected: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"GPU device not detected. Using CPU: {device}\")\n",
    "    \n",
    "\n",
    "\n",
    "def get_alpha_cumprod(t, steps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "    \"\"\"\n",
    "    计算alpha的累计乘积\n",
    "    \"\"\"\n",
    "    beta = torch.linspace(beta_start, beta_end, steps)\n",
    "    alpha = 1 - beta\n",
    "    alpha_cumprod = torch.cumprod(alpha, dim=0)\n",
    "    return alpha_cumprod[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "dataset = ESM2Dataset(esm_encoding_dir=Path(\"data/esm_encodings\"))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 每个样本是 (input_tensor, target_tensor)\n",
    "    input_seqs = [item[0] for item in batch]\n",
    "    target_seqs = [item[1] for item in batch]\n",
    "\n",
    "    input_lengths = torch.tensor([len(seq) for seq in input_seqs])\n",
    "    target_lengths = torch.tensor([len(seq) for seq in target_seqs])\n",
    "\n",
    "    # 两个序列的长度是一样的，可以统一 pad 成 max_len\n",
    "    # 你也可以分别 pad（如果以后变成不一样长）\n",
    "\n",
    "    input_padded = pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_padded = pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return input_padded, input_lengths, target_padded, target_lengths\n",
    "    \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "for esm, epitope, esm_len, epitope_len in dataloader:\n",
    "    print(esm.shape)\n",
    "    print(epitope.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c248db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02, device=device):\n",
    "        self.timesteps = timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.device = device\n",
    "        \n",
    "        # 线性噪声调度（公式4）\n",
    "        self.betas = torch.linspace(self.beta_start, self.beta_end, self.timesteps, device=self.device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)  # ᾱ_t（公式4推导）\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)  # √ᾱ_t\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)  # √(1-ᾱ_t)\n",
    "        \n",
    "        # 后验q(x_{t-1}|x_t,x_0)的计算（公式7）\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "\n",
    "    def extract(self, a, t, x_shape):\n",
    "        \"\"\"从a中根据t提取系数并重塑使其能与x_shape广播\"\"\"\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.gather(-1, t)\n",
    "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"前向扩散过程：q(x_t | x_0)（公式4推导）\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "            \n",
    "        sqrt_alphas_cumprod_t = self.extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14e9ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "class MyDenseNetWithSeqLen(nn.Module):\n",
    "    def __init__(self,\n",
    "                 esm_embedding_size = 1281,\n",
    "                 fc1_size = 150,\n",
    "                 fc2_size = 120,\n",
    "                 fc3_size = 45,\n",
    "                 fc1_dropout = 0.7,\n",
    "                 fc2_dropout = 0.7,\n",
    "                 fc3_dropout = 0.7,\n",
    "                 num_of_classes = 2):\n",
    "        super(MyDenseNetWithSeqLen, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.esm_embedding_size = esm_embedding_size\n",
    "        self.fc1_size = fc1_size\n",
    "        self.fc2_size = fc2_size\n",
    "        self.fc3_size = fc3_size\n",
    "        self.fc1_dropout = fc1_dropout\n",
    "        self.fc2_dropout = fc2_dropout\n",
    "        self.fc3_dropout = fc3_dropout\n",
    "        \n",
    "        self.ff_model = nn.Sequential(nn.Linear(esm_embedding_size, fc1_size),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(fc1_dropout),\n",
    "                                      nn.Linear(fc1_size, fc2_size),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(fc2_dropout),\n",
    "                                      nn.Linear(fc2_size, fc3_size),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(fc3_dropout),\n",
    "                                      nn.Linear(fc3_size, num_of_classes))\n",
    "    \n",
    "    def forward(self, antigen):\n",
    "        batch_size = antigen.size(0)\n",
    "        seq_len = antigen.size(1)\n",
    "        print(antigen.shape)\n",
    "        #convert dim (N, L, esm_embedding) --> (N*L, esm_embedding)\n",
    "        output = torch.reshape(antigen, (batch_size*seq_len, self.esm_embedding_size))\n",
    "        output = self.ff_model(output)                                               \n",
    "        return output\n",
    "\n",
    "\n",
    "class BepiPredDDPM(nn.Module):\n",
    "    def __init__(self, esm_embedding_size=1280, timestep_dim=64):\n",
    "        super().__init__()\n",
    "        # 时间步嵌入层\n",
    "        self.timestep_embed = nn.Sequential(\n",
    "            nn.Linear(timestep_dim, esm_embedding_size + 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(esm_embedding_size+1, esm_embedding_size+1)\n",
    "        )\n",
    "        \n",
    "        # 继承 BepiPred-3.0 的 FFNN\n",
    "        self.ffnn = MyDenseNetWithSeqLen()  # 输入拼接时间步信息\n",
    "\n",
    "        \n",
    "        self.classifier = nn.Linear(esm_embedding_size+1, 1)  # 二分类\n",
    "        \n",
    "\n",
    "    def get_time_embedding(self, t, dim):\n",
    "        # t: 时间步张量, shape=(batch_size, 1)\n",
    "        # dim: 嵌入向量的维度\n",
    "        half_dim = dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "        emb = t.float() * emb.unsqueeze(0).to(device)  \n",
    "        # emb.shape ==> (batch_size, half_dim)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)  # shape=(batch_size, dim)\n",
    "        return emb.to(device)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x: 噪声化ESM嵌入 [B, L, D]\n",
    "        # t: 时间步 [B]\n",
    "        t = t.unsqueeze(-1)\n",
    "        t_embed = self.get_time_embedding(t, dim=64)  # [B, D]\n",
    "        t_embed = self.timestep_embed(t_embed)\n",
    "        t_embed = t_embed.unsqueeze(1).expand(-1, x.size(1), -1)  # [B, L, D]\n",
    "        # print(t_embed.shape, x.shape)\n",
    "        x = t_embed + x\n",
    "        # x = torch.cat([x, t_embed], dim=-1)\n",
    "        print(x.shape)\n",
    "        noise_pred = self.ffnn(x)  # 预测噪声 [B, L, D]\n",
    "        # 若需联合表位分类\n",
    "        print(x.shape)\n",
    "        \n",
    "        epitope_prob = torch.sigmoid(self.classifier(x))  # [B, L, 1]\n",
    "        return noise_pred, epitope_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d32c89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model = BepiPredDDPM()\n",
    "diffusion = Diffusion()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a0430312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 500, 1281])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mb/58trsstd19510mh5cpw6ckfw0000gn/T/ipykernel_59541/929776122.py:23: UserWarning: Using a target size (torch.Size([20, 500, 1281])) that is different to the input size (torch.Size([10000, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_diffusion = F.mse_loss(pred_noise, noise)\n",
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 500, 1281])\n",
      "torch.Size([20, 500, 1281])\n",
      "torch.Size([20, 500, 1281])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (1281) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 3. 预测噪声并计算损失\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pred_noise, epitope_prob \u001b[38;5;241m=\u001b[39m model(xt, t)\n\u001b[0;32m---> 23\u001b[0m loss_diffusion \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_noise, noise)\n\u001b[1;32m     24\u001b[0m loss_epitope \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(epitope_prob, epitope_labels)  \u001b[38;5;66;03m# 需提供真实表位标签\u001b[39;00m\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_diffusion \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m loss_epitope  \u001b[38;5;66;03m# 加权平衡\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (1281) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# def train(model, dataloader, optimizer, steps=1000, device=device, epochs=10):\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "    loss_record= []\n",
    "    for x0, epitope_labels, x0_len, epitope_len in progress_bar:  # x0: 真实ESM嵌入 [B, L, D]\n",
    "        print(x0.shape)\n",
    "        \n",
    "        # epitope_labels = epitope_labels.to(device)\n",
    "        # x0 = x0.to(device)\n",
    "        \n",
    "        # 1. 随机采样时间步和噪声\n",
    "        t = torch.randint(0, steps, (x0.size(0),), device=device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        \n",
    "        # 2. 前向加噪（根据噪声调度）\n",
    "        \n",
    "        xt = diffusion.q_sample(x0, t)\n",
    "        t = t.float()\n",
    "        # 3. 预测噪声并计算损失\n",
    "        pred_noise, epitope_prob = model(xt, t)\n",
    "        loss_diffusion = F.mse_loss(pred_noise, noise)\n",
    "        loss_epitope = F.binary_cross_entropy(epitope_prob, epitope_labels)  # 需提供真实表位标签\n",
    "        total_loss = loss_diffusion + 0.1 * loss_epitope  # 加权平衡\n",
    "        \n",
    "        # 4. 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        loss_record.append(total_loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. 记录损失\n",
    "        progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "\n",
    "        # 每个 epoch 结束后打印损失并保存\n",
    "        print(f\"Epoch {epoch + 1}, Loss_Mean: {torch.tensor(loss_record).mean()}\", end=\"\\r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 500, 1281])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7bb686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
